---
title: "HarvardX Capstone Project - MovieLens Report"
author: "Leandro Rodrigues Carvalho"
date: "09/02/2021"
output:
  pdf_document: default
  html_document: default
---

# Introduction

This is an R Markdown report about on the "MovieLens" Project, developed as part of Harvard University online Professional Certificate course on Data Science, lead by Professor Rafael A. Irizarry. The *goal* of this project is to create a movie recommendation system to predict movie ratings using the MovieLens data set. The *key steps* that were performed included Exploratory Data Analysis (EDA), data visualization, data wrangling and training the machine learning algorithm using the inputs in one subset to predict movie ratings in the validation set.

# Historic Context

The recommendation system presented at the HarvardX course went through some of the data analysis strategies used by the winning team of the 2006 "Netflix Challenge", when the company challenged the data science community to improve its recommendation algorithm by 10%. The winner would get a million dollars prize. In September 2009, the winners were announced.

# Code provided

This code creates a training set and a validation, or test, set (final hold-out test set)

```{r message=FALSE, warning=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
```

#MovieLens 10M dataset:
#https://grouplens.org/datasets/movielens/10m/
#http://files.grouplens.org/datasets/movielens/ml-10m.zip

```{r message = FALSE, warning = FALSE}
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)

colnames(movies) <- c("movieId", "title", "genres")

# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

## Saving data as R objects

```{r message = FALSE, warning = FALSE}
save(edx, file = 'edx.RData')
save(validation, file = 'validation.RData')
```

# Dataset description

The data set version of MovieLens used in this report, the 10M version of the original data set, is a small subset of a much larger set with millions of ratings, in order to make the computation faster.

It's important to mention that the Netflix data is not publicly available, but the GroupLens research lab generated their own database with over 20 million ratings for over 27,000 movies by more than 138,000 users.

HarvardX and Professor Rafael A. Irizarry made a small subset of this data via the dslabs package containing 9,000,055 observations of 6 variables, where each row represents a rating given by one user to one movie.

```{r eval = FALSE}
str(edx)
```

There is a total of 10,677 different movies and 69,878 different users in the data set.

```{r}
unique_col <- edx %>%
summarize(unique_users = n_distinct(userId),
unique_movies = n_distinct(movieId),
unique_genres = n_distinct(genres))

knitr::kable(unique_col)
```

```{r}
tot_observation <- length(edx$rating) + length(validation$rating) 
tot_observation
```

Some of the most popular genres in the data set are "Drama", "Comedy", "Thriller" and "Romance".

```{r}
genres = c("Drama", "Comedy", "Thriller", "Romance")
  sapply(genres, function(g) {
    sum(str_detect(edx$genres, g))
  })
```

The movie with the greatest number of ratings is "Pulp Fiction"

```{r message = FALSE, warning = FALSE}
edx %>% group_by(movieId, title) %>%
	summarize(count = n()) %>%
	arrange(desc(count))
```

It's important to understand that not every user rated every movie. There are users on the rows and movies on the columns with many empty cells. 

```{r}
keep <- edx %>%
     dplyr::count(movieId) %>%
     top_n(5) %>%
     pull(movieId)

tab <- edx %>%
     filter(userId %in% c(13:20)) %>% 
     filter(movieId %in% keep) %>% 
     select(userId, title, rating) %>% 
     spread(title, rating)

tab %>% knitr::kable()
```

### Ratings profile

```{r}
table_rating <- as.data.frame(table(edx$rating))
colnames(table_rating) <- c('Rating', 'Frequencies')

knitr::kable(table_rating)
```

Visualizing the ratings

```{r echo = FALSE}
table_rating %>% ggplot(aes(Rating, Frequencies)) +
geom_bar(stat = 'identity') +
labs(x='Ratings', y='Count') +
ggtitle('Distribution of ratings')
```

# Methods and Analysis

Initially, we have to create a test set using the **createDataPartition** function to make possible to assess the accuracy of the model we'll create. In this project, 10% of the data is assign to the test set and 90% to the train set.

## Residual Mean Squared Error (RMSE)

Part important of this project is to provide the RMSE using only the training set (edx), and experimenting with multiple parameters. That strategy follows the Netflix challenge winning project that based their work on the residual mean squared error (RMSE) on the test set.

As Professor Rafael A. Irizarry explained in the course, *"we can interpret the RMSE similarly to a standard deviation: it is the typical error we make when predicting a movie rating. If this number is larger than 1, it means our typical error is larger than one star, which is not good".* 

The first thing to do is to develop a function to compute the RMSE for vectors of ratings and their corresponding predictors.

```{r}
RMSE <- function(true_ratings, predicted_ratings){
sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

# Building up the model step-by-step

## Step One

The ground base model is the simplest possible recommendation system that assumes the same rating for all movies. Considering that the estimate that minimizes the RMSE is the least squares estimate of u or the average of all rating.

```{r}
mu_hat <- mean(edx$rating)
mu_hat
```

Predicting all unknown ratings with u hat we obtain the following RMSE

```{r}
step_one_rmse <- RMSE(validation$rating, mu_hat)
step_one_rmse
```

```{r}
rmse_project_results <- data_frame(Method = "Step One/Base Model", RMSE = step_one_rmse)

rmse_project_results %>% knitr::kable()
```

## Step Two

The second step aiming to develop the model is to add an 'effect' (or an 'bias', as referred in the Netflix challenge) to represent **average ranking** for movie i. We can estimate b(i), or the bias of film "i" using least squares.

#lm(rating ~ as.factor(movieID), data = movielens)

The use of linear regression and the lm() function would demand too much computation and time so we will consider that the least squares estimate b(i) is just the average of Y(u,i) - u_hat and opt for the code below

```{r message = FALSE, warning = FALSE}
mu <- mean(edx$rating)

movie_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))
```

We can visualize the penalty term on the movie effect in a skewed histogram:

```{r echo = FALSE} 
movie_avgs %>% qplot(b_i, geom ="histogram", bins = 20, data = ., color = I("black"))
```

When we plug in the base formula of Y_hat(u, i) = U_hat + b_hat(i) we have

```{r}
predicted_ratings_movie_bias <- mu + validation %>%
  left_join(movie_avgs, by = 'movieId') %>%
  pull(b_i)

step_two_rmse <- RMSE(predicted_ratings_movie_bias, validation$rating)
step_two_rmse
```

```{r}
rmse_project_results <- bind_rows(rmse_project_results, data_frame(Method = "Step Two/Movie Bias",
RMSE = step_two_rmse))

rmse_project_results %>% knitr::kable()
```

## Step Three

To further improve the model, we can do something similar to what we did in *Step Two*, but this time with 'user bias', adding to the formula a "user-specific effect" (b_hat(u)). As before, we won't use linear regression because the lm() function will be very slow due to big number of bias (bs). Instead, we will use an approximation by computing u_hat and b_hat(i), and estimating b_hat(u) as the average of 

y(u, i) - u_hat - b_hat(i)

```{r warning = FALSE, message = FALSE}
user_avgs <- edx %>%
  left_join(movie_avgs, by = 'movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
```

We can visualize the penalty term on the user effect in a histogram:

```{r echo = FALSE}
user_avgs %>% qplot(b_u, geom ="histogram", bins = 30, data = ., color = I("black"))
```

Now we can apply it to build up a predictor and check if it improves the RMSE.

```{r message = FALSE, warning = FALSE}
predicted_ratings_user_bias <- validation %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

step_three_rmse <- RMSE(predicted_ratings_user_bias, validation$rating)
step_three_rmse
```

Yes, it does!

```{r}
rmse_project_results <- bind_rows(rmse_project_results, data_frame(Method = "Step Three/User Bias",
RMSE = step_three_rmse))

rmse_project_results %>% knitr::kable()
```

# Step Four

Despite the latest improvements, the performance of the model, so far, if applied to the test set, would rate unknown movies on the top, showing that we haven't achieved a good result yet.This happens because some movies have only a few ratings, from a few users, implying more uncertainty. As Professor Irizarry explained, larger estimates of b(i), negative or positive, are more likely. *"These are noisy estimates that we should not trust, especially when it comes to prediction. Large errors can increase our RMSE, so we would rather be conservative when unsure".*

### Regularization

Here is when he introduces the concept of **regularization**, that permits us to penalize large estimates that are formed using small sample sizes, constraining the total variability of the effect sizes.

*"The general idea of penalized regression is to control the total variability of the movie effects. Specifically, instead of minimizing the least squares equation, we minimize an equation that adds a penalty".*

When our sample size is large, a case which will give us a stable estimate, then the penalty *lambda* is effectively ignored since n(i) + lambda = n. However, when the n(i) is small, then the estimate b_hat(i)(lambda) is shrunken towards 0. The larger lambda,the more we shrink.

Computing the regularized estimates of b(i) using lambda = 3 we have

```{r message = FALSE, warning = FALSE}
lambda <- 3

mu <- mean(edx$rating)

movie_reg_avgs <- edx %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n())
```

The following plot of the regularized estimates versus the least squares estimates show how the estimates shrink.

```{r echo = FALSE}
data_frame(original = movie_avgs$b_i, 
           regularlized = movie_reg_avgs$b_i, 
           n = movie_reg_avgs$n_i) %>%
     ggplot(aes(original, regularlized, size=sqrt(n))) + 
     geom_point(shape=1, alpha=0.5)
```

To further explore lambda as a tuning parameter we can use **cross-validation**.

```{r message = FALSE, warning = FALSE}
lambdas <- seq(0, 10, 0.25)

mu <- mean(edx$rating)

just_the_sum <- edx %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())

rmses <- sapply(lambdas, function(l){
  predicted_ratings <- validation %>% 
    left_join(just_the_sum, by = 'movieId') %>% 
    mutate(b_i = s/(n_i+l)) %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, validation$rating))
})
```

Plotting the result

```{r echo = FALSE}
qplot(lambdas, rmses)
```

And figuring out the which.min rmse

```{r}
lambdas[which.min(rmses)]
```

As this is shown in the course as a illustrative procedure, it's important to note that we should always use full cross-validation just on the train set, without using the test set until the final assessment. **The test set should never be used for tuning.**

We can use regularization for the estimate *user effects* as well. The estimates that minimizes this can be found similarly to what we did above. Here we use cross-validation to pick a lambda.

```{r message = FALSE, warning = FALSE}
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){

  mu <- mean(edx$rating)
  
  b_i <- edx %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- edx %>% 
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))

  predicted_ratings <- validation %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
    return(RMSE(predicted_ratings, validation$rating))
})
```

To visualize

```{r echo = FALSE}
ggplot(data.frame(lambdas = lambdas, rmses = rmses ), aes(lambdas, rmses)) +
geom_point()
```

For the full model, the optimal lambda is:

```{r}
lambda <- lambdas[which.min(rmses)]
lambda
```

# Results and Conclusion

### RMSE Project Results
```{r}
rmse_project_results <- bind_rows(rmse_project_results,
                          data_frame(Method = "Step Four/Regularized Movie + User Effect Model",  
                                     RMSE = min(rmses)))

rmse_project_results %>% knitr::kable()
```

The base model assumes the same rating for all movies and doesn't achieve a good enough result: its RMSE is more than 1, or in other words, an error of an entire star! To improve this result we added an 'effect' (or bias) representing the average ranking for *movie i* on Step Two and, following the same logic, an effect for *user u* on Step Three.

Step Three achieved a good improvement on the performance of the model with an RMSE of 0.8653488.

The problem here, though,is that when we apply it to the test set, it rates unknown movies on the top, showing that something wasn't quite right.

To sort this out, we've moved ahead to Step Four, applying the concept of **regularization**, penalizing large estimates that are formed using small sample sizes, and adjusting for noisy estimates that were withholding the model to perform better, reaching a final RMSE of 0.864817. >